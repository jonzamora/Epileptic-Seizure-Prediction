{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Practice with EEG Analysis in Python\n",
    "\n",
    "### Overview\n",
    "\n",
    "The purpose of this task is to give you a chance to get familiar with some techniques for analyzing EEG signals using Python. Our goal will be to predict whether or not a segment of data from an EEG corresponds to a seizure or not. Our analysis will consist of three main steps which are described in detail below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Reading the data\n",
    "\n",
    "The data for this project comes from the \"Physionet\" database collected by the Children's Hospital of Boston. The data consits of EEG recordings gathered from 10 children and young adults with medically resistant epilepsy. Each test subject has around 50 hours of recordings so there's a lot of data! A neurologist then went through all the data by hand and identified all the seizures. So each sample gets a value of `1` if it corresponds to a seizure and a `0` if not. The data was collected in a hospital setting and more subjects have data from at least 20 different electrodes. However, to be closer to the environment faced by developers of embedded systems we'll only look at two channels that are located near the temples. You can see some more detailed information about the data [here](https://physionet.org/content/chbmit/1.0.0/).\n",
    "\n",
    "In the `../input/` folder you'll find a file `eeg_data_temples2.h5` which contains all the EEG recordings for the two channels we're interested in. Our first step will be to read this data into Python and convert it into a format our algorithm can work with. To do so, we'll use the `tables` library for Python which can read the `HDF5` file format which is used to store the EEG data.\n",
    "\n",
    "## Side Note: Model Estimation and Feature Extraction\n",
    "\n",
    "For this task we want to classify a short segment (commonly called a \"window\") of an EEG recording (4 seconds to be exact) as either being a during a seizure or not. To do so, we will use a logistic regression model. Very succinctly, a lostic regression is a statistical model which can be used predicts the value of a binary random variable (i.e. a random variable which takes on exactly two discrete values) known as the \"outcome\" or \"dependent variable,\" given a set of observations of one or more \"input\" or \"independent\" variables. Very succinctly, suppose we have two input variables $x_1 \\in \\mathbb{R}$ and $x_2 \\in \\mathbb{R}$ (where $\\mathbb{R}$ denotes the real numbers) and a discrete outcome $y \\in \\{0,1\\}$. Then logistic regression first computes a linear function of those variables:\n",
    "\n",
    "$$ z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 $$\n",
    "\n",
    "and then \"squashes\" $z$ through a function (called the logistic-sigmoid) which converts $z$ into a probability. Putting both steps together we have:\n",
    "\n",
    "$$ Pr(y = 1 | x_1, x_2) = \\sigma(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2) $$\n",
    "\n",
    "where $\\sigma(x) = \\frac{1}{1+e^{-x}}$ is the logistic sigmoid. In general, we don't know $\\beta_0, \\beta_1, \\beta_2$ and need to *estimate* them from some data. The $\\beta$ are known as the model \"parameters\" and the process of estimating them is called \"parameter estimation\" (there's an entire ECE course that is just about parameter estimation). The algorithm to do this is somewhat complicated, but fortunately some other people have already implemented it for us. We just need to give their implementation a bunch of data corresponding to several observed values of $x_1$ and $x_2$ along with the correct value of $y$ for those data points. Their algorithm will give us back the \"best\" values of $\\beta$. Then, if we observe values for $x_1$ and $x_2$ for which we *do not* know the correct value of $y$, we can plug those into the equation above along with the estimated values for the $\\beta$ and compute the probabilty that these $x$ correspond to $y=1$. \n",
    "\n",
    "This is a very over-simplified description of logistic regression which has a number of interesting interpretations both statistically and geometrically which you're encouraged to read more about [here](https://en.wikipedia.org/wiki/Logistic_regression). In particular, I recommend reading the first two \"examples\" to get a bit more technical understanding of how the model works and how we define the \"best\" value of the $\\beta$.\n",
    "\n",
    "In our case the outcome variable will be \"given the last four seconds of EEG data, is the person having a seizure?\" So in this case our $x$ variables will be some variables which summarize the last four seconds of observed EEG data and our $y$ variable will be a $1$ if the person was having a seizure at any point in the last four seconds. The question of how to best summarize the signal so that a logistic regression gives good results has been the subject of considerable research and people have proposed a bunch of different values to extract from the seizure which may be useful. In this case we'll use a very simple model that uses the mean and variance of the signal in each window along with some features of the \"frequency domain\" which are well known to be useful by medical researchers. The term \"frequency domain\" refers to the representation of the signal obtained by computing its Fourier Transform. Recall that the [Fourier Transform](https://en.wikipedia.org/wiki/Fourier_transform) takes a signal which is measured as a function of time and decomposes it into a superposition of primitive waves oscilating at different frequencies. In many cases (including this one), the relative importance of these waves tells us useful things about the signal so it's very common to analyze signals in the frequency domain. If you don't know about Fourier Transforms, that's fine, but I recommend reading about them (or look up the 3Blue1Brown video about them) because they are very interesting.\n",
    "\n",
    "So our first task will be to read the raw data collected on EEG and extract the various \"features\" from the raw signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tables\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import signal\n",
    "from scipy import integrate\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "h5_file = tables.open_file(\"/Users/jzamora/Downloads/eeg-analysis-ra/input/eeg_data_temples2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function computes the relative power in several frequency bands which\n",
    "# are generally known to be medically relevant\n",
    "\n",
    "def compute_band_relpower(X):\n",
    "    band_relpower = []\n",
    "    bands = [(0.5,4), (4,8), (8,13), (13,32), (32,60)]\n",
    "\n",
    "    freqs, psd = signal.welch(X, fs=256.0, axis=0)\n",
    "    fr_res = freqs[1] - freqs[0]\n",
    "\n",
    "    where = lambda lb, ub: np.logical_and(freqs >= lb, freqs < ub)\n",
    "    abs_power = np.concatenate(\n",
    "            [integrate.simps(psd[where(lb, ub),:], dx = fr_res, axis = 0\n",
    "                ).reshape(1,-1) for lb, ub in bands]\n",
    "        )\n",
    "    total_power = integrate.simps(psd, dx=fr_res, axis=0).astype(np.float64)\n",
    "    abs_power[:,total_power == 0] = 0\n",
    "    total_power[total_power == 0] = -1\n",
    "    band_relpower = (abs_power / total_power).ravel()    \n",
    "\n",
    "    return band_relpower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jzamora/Downloads/eeg-analysis-ra/input/eeg_data_temples2.h5 (File) ''\n",
      "Last modif.: 'Sat Feb  8 23:57:08 2020'\n",
      "Object Tree: \n",
      "/ (RootGroup) ''\n",
      "/chb01 (Group) ''\n",
      "/chb01/chb01_01 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_02 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_03 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_04 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_05 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_06 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_07 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_08 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_09 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_10 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_11 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_12 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_13 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_14 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_15 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_16 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_17 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_18 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_19 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_20 (CArray(681726, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_21 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_22 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_23 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_24 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_25 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_26 (CArray(595198, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_27 (CArray(153598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_29 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_30 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_31 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_32 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_33 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_34 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_36 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_37 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_38 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_39 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_40 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_41 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_42 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_43 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb01/chb01_46 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03 (Group) ''\n",
      "/chb03/chb03_01 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_02 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_03 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_04 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_05 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_06 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_07 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_08 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_09 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_10 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_11 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_12 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_13 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_14 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_15 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_16 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_17 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_18 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_19 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_20 (CArray(923134, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_21 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_22 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_23 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_24 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_25 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_26 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_27 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_28 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_29 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_30 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_31 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_32 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_33 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_34 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_35 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_36 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_37 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb03/chb03_38 (CArray(921598, 3), shuffle, blosc(9)) ''\n",
      "/chb10 (Group) ''\n",
      "/chb10/chb10_01 (CArray(1848318, 3), shuffle, blosc(9)) ''\n",
      "/chb10/chb10_02 (CArray(1843198, 3), shuffle, blosc(9)) ''\n",
      "/chb10/chb10_03 (CArray(1843198, 3), shuffle, blosc(9)) ''\n",
      "/chb10/chb10_04 (CArray(1843198, 3), shuffle, blosc(9)) ''\n",
      "/chb10/chb10_05 (CArray(1843198, 3), shuffle, blosc(9)) ''\n",
      "/chb10/chb10_06 (CArray(1843198, 3), shuffle, blosc(9)) ''\n",
      "/chb10/chb10_07 (CArray(1843198, 3), shuffle, blosc(9)) ''\n",
      "/chb10/chb10_08 (CArray(1843198, 3), shuffle, blosc(9)) ''\n",
      "/chb10/chb10_12 (CArray(1843198, 3), shuffle, blosc(9)) ''\n",
      "/chb10/chb10_13 (CArray(1843198, 3), shuffle, blosc(9)) ''\n",
      "/chb10/chb10_14 (CArray(1843198, 3), shuffle, blosc(9)) ''\n",
      "/chb10/chb10_15 (CArray(1843198, 3), shuffle, blosc(9)) ''\n",
      "/chb10/chb10_16 (CArray(1843198, 3), shuffle, blosc(9)) ''\n",
      "/chb10/chb10_17 (CArray(1843198, 3), shuffle, blosc(9)) ''\n",
      "/chb10/chb10_18 (CArray(1843198, 3), shuffle, blosc(9)) ''\n",
      "/chb10/chb10_19 (CArray(1843198, 3), shuffle, blosc(9)) ''\n",
      "/chb10/chb10_20 (CArray(1846270, 3), shuffle, blosc(9)) ''\n",
      "/chb10/chb10_21 (CArray(1843198, 3), shuffle, blosc(9)) ''\n",
      "/chb10/chb10_22 (CArray(1843198, 3), shuffle, blosc(9)) ''\n",
      "/chb10/chb10_27 (CArray(1847806, 3), shuffle, blosc(9)) ''\n",
      "/chb10/chb10_28 (CArray(1843198, 3), shuffle, blosc(9)) ''\n",
      "/chb10/chb10_30 (CArray(1846526, 3), shuffle, blosc(9)) ''\n",
      "/chb10/chb10_31 (CArray(1848574, 3), shuffle, blosc(9)) ''\n",
      "/chb10/chb10_38 (CArray(1843198, 3), shuffle, blosc(9)) ''\n",
      "/chb10/chb10_89 (CArray(1843198, 3), shuffle, blosc(9)) ''\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the file. It contains several \"nodes\" each of which corresponds to one test subject\n",
    "# Each node has several children which each correspond to (roughly) one hour of data.\n",
    "\n",
    "print(h5_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll read the data into a Python data structure and \n",
    "# extract features of the signal to be used in a statistical \n",
    "# algorithm for prediction\n",
    "\n",
    "sampling_rate = 256     # how many observations (samples) are gathered every? second\n",
    "window_size_seconds = 4 # how many seconds of data do we want in a window?\n",
    "stride_seconds = 2      # how far should we advance the window at each step?\n",
    "\n",
    "window_size = window_size_seconds * sampling_rate\n",
    "stride = stride_seconds * sampling_rate\n",
    "\n",
    "features = []\n",
    "labels   = []\n",
    "subjects = []\n",
    "\n",
    "\n",
    "for node in h5_file.walk_nodes(\"/\", \"CArray\"):\n",
    "    node_data = node.read()\n",
    "    subject_id = node._v_name.split(\"/\")[0].split(\"_\")[0]\n",
    "    \n",
    "    # The data for each node is an N x 3 numpy array (a matrix)\n",
    "    # The first two columns are the EEG data and the third is an\n",
    "    # indicator variable which is equal to 1 if the observation \n",
    "    # corresponds to a seizure\n",
    "    \n",
    "    X, y = node_data[:, :-1], node_data[:, -1]\n",
    "    num_obs, num_channels = X.shape\n",
    "    \n",
    "    # Now we want to convert our data matrix (X) into a sequence\n",
    "    # of overlapping windows\n",
    "    for ix in range(window_size, num_obs, stride):\n",
    "        X_w = X[ix-window_size:ix, :]\n",
    "                \n",
    "        # now let's extract the \"features\" mentioned above\n",
    "        # TODO: compute the mean and variance of both channels of data\n",
    "        # hint: use np.mean and np.var and look up the \"axis\" argument\n",
    "        band_relpower = compute_band_relpower(X_w)\n",
    "        means = np.mean(X_w, axis = 0)\n",
    "        variances = np.var(X_w, axis = 0)\n",
    "        \n",
    "        # TODO: concatenate all features into a single \"feature vector\"\n",
    "        # hint: use np.concatenate\n",
    "        feature_vector = np.concatenate((band_relpower, means, variances), axis = 0)\n",
    "        features.append(feature_vector)\n",
    "        \n",
    "        labels.append(np.any(y[ix-window_size:ix]))\n",
    "        subjects.append(subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can concatenate all the feature vectors into one big matrix:\n",
    "X = np.vstack(features)\n",
    "y = np.array(labels)\n",
    "subjects = np.array(subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Split Data into Train and Test Sets\n",
    "\n",
    "We're now almost ready to pass our data (X) to the algorithm to estimate the model's parameters. But what if we got lucky and collected a bunch of data that's really easy to classify correctly? Then, our model will appear to be much better than it actually is. What we're really interested in is: how well does our model predict data on which it was not trained? A simple way to answer this question is to hold out a sub-sample of your data from training and then evaluate the accuracy of the model (how often it guesses the correct value) on this held out sample. This is known as \"cross validation.\"\n",
    "\n",
    "For medical applications where you often have multiple test subjects, the best way to perform cross validation is to exclude one subject from training and use that subject as the validation sample. The model is trained on everyone else and we use the test subject to estimate the model performance. So in this step, we want to split the data into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Size: 231231\n",
      "X Train: (158320, 14) Data Type: float64 Array Type: <class 'numpy.ndarray'>\n",
      "Y Train: (158320,) Data Type: bool Array Type: <class 'numpy.ndarray'>\n",
      "X Test (72911, 14) Data Type float64\n",
      "Y Test (72911,) Data Type bool\n"
     ]
    }
   ],
   "source": [
    "is_test_set = (subjects == \"chb01\")\n",
    "print('Test Set Size:', len(is_test_set))\n",
    "X_train = X[np.logical_not(is_test_set), :]\n",
    "y_train = y[np.logical_not(is_test_set)]\n",
    "\n",
    "print('X Train:', X_train.shape, 'Data Type:', X_train.dtype, 'Array Type:', type(X_train))\n",
    "print('Y Train:', y_train.shape, 'Data Type:', y_train.dtype, 'Array Type:', type(y_train))\n",
    "\n",
    "# TODO: use the \"is_test_set\" variable to extract the rows of `X`\n",
    "# corresponding to the test subject.\n",
    "X_test = X[is_test_set, :]\n",
    "y_test = y[is_test_set]\n",
    "\n",
    "print('X Test', X_test.shape, 'Data Type', X_test.dtype)\n",
    "print('Y Test', y_test.shape, 'Data Type', y_test.dtype)\n",
    "\n",
    "# As a final step, we'll resacle each of our features so that they\n",
    "# have a mean of zero and a standard deviation of one. Again, the \n",
    "# reason why we do this is a bit technical, but in general ML models\n",
    "# don't like it when the features are on different scales.\n",
    "\n",
    "S = StandardScaler()\n",
    "X_train = S.fit_transform(X_train)\n",
    "X_test = S.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Estimate Model Parameters\n",
    "\n",
    "We're now finally ready to estimate the parameters of our model. Fortunately, this step is really easy. We just construct a `LogisticRegression` object and call its `fit` method. All the hard work was done by the creators of `scikit-learn` who implemented the algorithm for parameter estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy was: 99.78903486609398%\n",
      "Test accuracy was: 99.63242857730657%\n"
     ]
    }
   ],
   "source": [
    "# TODO: construct a logistic regression object and call it's .fit()\n",
    "# method with the training data\n",
    "\n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "# Now let's evaluate the performance of our model on the train and\n",
    "# test set. We can use the \"predict\" method of the model to obtain\n",
    "# predictions and compare these against the correct values\n",
    "# TODO: compute the accuracy of the model on the training and testing\n",
    "# datasets\n",
    "\n",
    "y_pred_train = clf.predict(X_train)\n",
    "train_accuracy = clf.score(X_train, y_train)\n",
    "\n",
    "y_pred_test = clf.predict(X_test)\n",
    "test_accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Training accuracy was: {}%\".format(100 * train_accuracy))\n",
    "print(\"Test accuracy was: {}%\".format(100 * test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging...\n",
    "\n",
    "Wow! $99\\%$ accuracy on the test set! Amazing! We've solved the problem of seizure detection! Or have we...\n",
    "\n",
    "Let's compute the true negative and true positive rate for our classifier. I.e. how many examples that actually were `0` were correctly classified as `0` and how many that really were `1` were correctly classified as `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True negative rate: 99.63242857730657%\n",
      "True positive rate: 0.0%\n",
      "False negative rate: 0.32093922727709123%\n",
      "False positive rate: 0.0466321954163295%\n"
     ]
    }
   ],
   "source": [
    "# TODO: compute the \"confusion matrix\" for the model\n",
    "# (hint: use the \"confusion_matrix\" function) and then use this\n",
    "# to compute the true negative rate and true positive rate\n",
    "\n",
    "cfn = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Binary Classification of Confusion Matrix C\n",
    "# C(ij) = # of Observations in Group [i] and Predicted to be in Group [j]\n",
    "true_negatives = cfn[0][0] / len(y_test)\n",
    "false_negatives = cfn[1][0] / len(y_test)\n",
    "true_positives = cfn[1][1] / len(y_test)\n",
    "false_positives = cfn[0][1] / len(y_test)\n",
    "\n",
    "print(\"True negative rate: {}%\".format(true_negatives * 100))\n",
    "print(\"True positive rate: {}%\".format(true_positives * 100))\n",
    "print(\"False negative rate: {}%\".format(false_negatives * 100))\n",
    "print(\"False positive rate: {}%\".format(false_positives * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My true positive rate was $0\\%$ - our model **never** correctly predicted that someone had a seizure - which is what we care about! So even though the accuracy is really high we actually have a terrible model. Why did this happen? Let's compute the fraction of training examples that are a `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0028612935826174838\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only about $0.1\\%$ of the training data points are a `1`! Therefore, they contribute almost nothing to the model and it ignores them! Again the reality is more bit complicated but requires some technical details on how the training algorithm works. This is a common problem in ML called an \"unbalanced training set.\" Which, in general, means your classifier sees way more of one class than the other(s). There are many methods to deal with unbalanced training sets. The simplest (but not the best) way is to downsample the overrepresented class to obtain a balanced training set with equal numbers of `0`'s and `1`'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Train [Downsampled]: (906, 14) \n",
      "Y Train [Downsampled]: (906,)\n",
      "X Test [Updated]: (226, 14) \n",
      "Y Test [Updated]: (226,)\n",
      "Training accuracy was: 94.03973509933775%\n",
      "Test accuracy was: 84.95575221238938%\n",
      "True negative rate: 84.95575221238938%\n",
      "True positive rate: 0.0%\n",
      "False negative rate: 0.0%\n",
      "False positive rate: 15.04424778761062%\n"
     ]
    }
   ],
   "source": [
    "# TODO: draw a random sample of the negative (0) training examples\n",
    "# to ensure there are an equal number of 0's and 1's in the training\n",
    "# data and estimate the model again.\n",
    "\n",
    "# Step 1: Determine True (1) and False (0) Indices in y_train set\n",
    "index_true = []\n",
    "index_false = []\n",
    "for i in range(len(y_train)):\n",
    "    if(y_train[i] == True):\n",
    "        index_true.append(i)\n",
    "    else:\n",
    "        index_false.append(i)\n",
    "        \n",
    "\n",
    "# Step 2: Match contents of index_true with (1) X_train and (2) y_train contents \n",
    "# and append index_true values to X_train_ds and y_train_ds lists\n",
    "X_train_ds = []\n",
    "y_train_ds = []\n",
    "a = 0\n",
    "b = 0\n",
    "running = True\n",
    "while(running):\n",
    "    if(index_true[b] == index_true[-1]):\n",
    "        X_train_ds.append(X_train[a, :])\n",
    "        y_train_ds.append(y_train[a])\n",
    "        running = False\n",
    "    elif(a == index_true[b]):\n",
    "        X_train_ds.append(X_train[a, :])\n",
    "        y_train_ds.append(y_train[a])\n",
    "        a += 1\n",
    "        b += 1\n",
    "    else:\n",
    "        a += 1\n",
    "\n",
    "# Step 3: Balance lists with (1) y_train False values and (2) Corresponding X_Train data\n",
    "c = 0\n",
    "d = 0\n",
    "running = True\n",
    "while(running):\n",
    "    if(index_false[d] == index_false[452]):\n",
    "        X_train_ds.append(X_train[c, :])\n",
    "        y_train_ds.append(y_train[c])\n",
    "        running = False\n",
    "    elif(c == index_false[d]):\n",
    "        X_train_ds.append(X_train[c, :])\n",
    "        y_train_ds.append(y_train[c])\n",
    "        c += 1\n",
    "        d += 1\n",
    "    else:\n",
    "        c += 1\n",
    "\n",
    "# Step 4: Convert lists into np.ndarrays\n",
    "X_train_updated = np.array(X_train_ds)\n",
    "y_train_updated = np.array(y_train_ds)\n",
    "print('X Train [Downsampled]:', X_train_updated.shape, '\\nY Train [Downsampled]:', y_train_updated.shape)\n",
    "\n",
    "# Step 5: Create X_test and y_test subsets\n",
    "X_test_updated = X_test[:226, :]\n",
    "y_test_updated = y_test[:226]\n",
    "print('X Test [Updated]:', X_test_updated.shape, '\\nY Test [Updated]:', y_test_updated.shape)\n",
    "\n",
    "X_train_updated = S.fit_transform(X_train_updated)\n",
    "X_test_updated = S.transform(X_test_updated)\n",
    "\n",
    "clf_updated = LogisticRegression().fit(X_train_updated, y_train_updated)\n",
    "\n",
    "y_pred_train_updated = clf_updated.predict(X_train_updated)\n",
    "train_accuracy_updated = clf_updated.score(X_train_updated, y_train_updated)\n",
    "\n",
    "y_pred_test_updated = clf_updated.predict(X_test_updated)\n",
    "test_accuracy_updated = clf_updated.score(X_test_updated, y_test_updated)\n",
    "\n",
    "cfn_updated = confusion_matrix(y_test_updated, y_pred_test_updated)\n",
    "\n",
    "true_negatives_updated = 100 * (cfn_updated[0][0] / len(y_test_updated))\n",
    "false_negatives_updated = 100 * (cfn_updated[1][0] / len(y_test_updated))\n",
    "true_positives_updated = 100 * (cfn_updated[1][1] / len(y_test_updated))\n",
    "false_positives_updated = 100 * (cfn_updated[0][1] / len(y_test_updated))\n",
    "\n",
    "print(\"Training accuracy was: {}%\".format(100 * train_accuracy_updated))\n",
    "print(\"Test accuracy was: {}%\".format(100 * test_accuracy_updated))\n",
    "\n",
    "print(\"True negative rate: {}%\".format(true_negatives_updated))\n",
    "print(\"True positive rate: {}%\".format(true_positives_updated))\n",
    "print(\"False negative rate: {}%\".format(false_negatives_updated))\n",
    "print(\"False positive rate: {}%\".format(false_positives_updated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got about $63\\%$ - not great, but much better than before and better than guessing randomly so our model is doing something useful! Seizure detection is an extensively studied problem and there are dozens of papers written about different methods to do it. We used a very simple method and there are lots of tricks you can use to improve even a simple model like logistic regression. Some things you might consider trying:\n",
    "* We could also consider *interactions* between the features and powers of a feature (e.g. a polynomial). For instance maybe $\\mu_1\\dot\\mu_{2}$ is useful for the model.\n",
    "* EEG signals are very noisy meaning they contain a lot of flucutations which are more or less random and don't tell us anything useful. There are lots of techniques in the literature on denoising. One common one is to compute the fourier transform of the signal and zero out high-frequency components of the signal (> 60Hz).\n",
    "* Logistic regression is a fiarly simple model that requires manually extracting features from the signal. We could instead try a more complicated model which can \"learn\" the best features to extract. The most well known of this class of models are neural networks.\n",
    "\n",
    "### Some More Analysis...\n",
    "So far, we've only evaluated the performance of the model on a single test subject. We should repeat this process holding out each test subject to get a better sense of the models typical performance.\n",
    "\n",
    "TODO: Repeat the above analysis holding out each subject. Write a function which takes the ID of the test subject, constructs the train and test sets (subsampling the training set), estimates models and returns the true negative and true positive rates. Create a table showing the true negative and true positive rate for each subject.\n",
    "\n",
    "Finally: Pick the best model and plot the probability returned by the logistic regression model (on the test set) over time. The X-axis should represent time and the Y-axis should be the probability estimated by the model that the corresponding window is a seizure. Use different colors for seizure and non-seizure points and include a legend showing which color is which."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
